{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNAwezj7YcFs"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Phase transformations\n",
        "train_transforms = transforms.Compose([\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n"
      ],
      "metadata": {
        "id": "BYXlKOtEYrSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
      ],
      "metadata": {
        "id": "vcP89O81Y5oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsYCrwy-Y709",
        "outputId": "de48e9c8-85a9-4b62-f262-c38b183db2a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model2, self).__init__()\n",
        "\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=0),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.ReLU()\n",
        "        ) # INPUT 28*28 | OUTPUT 26*26 | RF 3\n",
        "\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=8, kernel_size=(3, 3), padding=0),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU()\n",
        "        ) # INPUT 26*26 | OUTPUT 24*24 | RF 5\n",
        "\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=8, kernel_size=(3, 3), padding=0),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU()\n",
        "        ) # INPUT 24*24 | OUTPUT 22*22 | RF 7\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # INPUT 22*22 | OUTPUT 11*11 | RF 8\n",
        "\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=20, kernel_size=(3, 3), padding=0),\n",
        "            nn.BatchNorm2d(20),\n",
        "            nn.ReLU()\n",
        "        ) # INPUT 11*11 | OUTPUT 9*9 | RF 12\n",
        "\n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=20, out_channels=16, kernel_size=(3, 3), padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU()\n",
        "        ) # INPUT 9*9 | OUTPUT 7*7 | RF 16\n",
        "\n",
        "\n",
        "        self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(3, 3), padding=0),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.ReLU()\n",
        "        ) # INPUT 7*7 | OUTPUT 5*5 | RF 20\n",
        "\n",
        "        self.convblock7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.ReLU()\n",
        "        ) # INPUT 5*5 | OUTPUT 3*3 | RF 24\n",
        "\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(3)\n",
        "        ) # INPUT 3*3 | OUTPUT 1*1 | RF 28\n",
        "\n",
        "        self.dropout = nn.Dropout(0.01) #added dropout\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.dropout(x)  #dropout\n",
        "        x = self.convblock3(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.dropout(x) #dropout\n",
        "        x = self.convblock6(x)\n",
        "        x = self.convblock7(x)\n",
        "        x = self.gap(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "axqX24YUZCg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Model2().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ezz40TynZEKU",
        "outputId": "715a9298-d284-4239-8fe8-c822d2b12a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 10, 26, 26]             100\n",
            "       BatchNorm2d-2           [-1, 10, 26, 26]              20\n",
            "              ReLU-3           [-1, 10, 26, 26]               0\n",
            "            Conv2d-4            [-1, 8, 24, 24]             728\n",
            "       BatchNorm2d-5            [-1, 8, 24, 24]              16\n",
            "              ReLU-6            [-1, 8, 24, 24]               0\n",
            "           Dropout-7            [-1, 8, 24, 24]               0\n",
            "            Conv2d-8            [-1, 8, 22, 22]             584\n",
            "       BatchNorm2d-9            [-1, 8, 22, 22]              16\n",
            "             ReLU-10            [-1, 8, 22, 22]               0\n",
            "        MaxPool2d-11            [-1, 8, 11, 11]               0\n",
            "           Conv2d-12             [-1, 20, 9, 9]           1,460\n",
            "      BatchNorm2d-13             [-1, 20, 9, 9]              40\n",
            "             ReLU-14             [-1, 20, 9, 9]               0\n",
            "           Conv2d-15             [-1, 16, 7, 7]           2,896\n",
            "      BatchNorm2d-16             [-1, 16, 7, 7]              32\n",
            "             ReLU-17             [-1, 16, 7, 7]               0\n",
            "          Dropout-18             [-1, 16, 7, 7]               0\n",
            "           Conv2d-19             [-1, 10, 5, 5]           1,450\n",
            "      BatchNorm2d-20             [-1, 10, 5, 5]              20\n",
            "             ReLU-21             [-1, 10, 5, 5]               0\n",
            "           Conv2d-22             [-1, 10, 3, 3]             910\n",
            "      BatchNorm2d-23             [-1, 10, 3, 3]              20\n",
            "             ReLU-24             [-1, 10, 3, 3]               0\n",
            "        AvgPool2d-25             [-1, 10, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 8,292\n",
            "Trainable params: 8,292\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.46\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.49\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "\n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "metadata": {
        "id": "nRxGiG5LZGW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =  Model2().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "EPOCHS = 15\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heLWUC0qZIFN",
        "outputId": "5cd05358-778a-4466-edab-a13e1eb5b115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12093589454889297 Batch_id=468 Accuracy=94.22: 100%|██████████| 469/469 [00:16<00:00, 28.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0962, Accuracy: 9843/10000 (98.43%)\n",
            "\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11352702230215073 Batch_id=468 Accuracy=98.36: 100%|██████████| 469/469 [00:16<00:00, 28.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0601, Accuracy: 9882/10000 (98.82%)\n",
            "\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08246047049760818 Batch_id=468 Accuracy=98.64: 100%|██████████| 469/469 [00:16<00:00, 27.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0479, Accuracy: 9905/10000 (99.05%)\n",
            "\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.030631719157099724 Batch_id=468 Accuracy=98.89: 100%|██████████| 469/469 [00:17<00:00, 27.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0388, Accuracy: 9916/10000 (99.16%)\n",
            "\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04610821604728699 Batch_id=468 Accuracy=98.99: 100%|██████████| 469/469 [00:16<00:00, 28.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0380, Accuracy: 9915/10000 (99.15%)\n",
            "\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.029482951387763023 Batch_id=468 Accuracy=99.09: 100%|██████████| 469/469 [00:16<00:00, 28.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0341, Accuracy: 9924/10000 (99.24%)\n",
            "\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.047235745936632156 Batch_id=468 Accuracy=99.14: 100%|██████████| 469/469 [00:17<00:00, 26.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0297, Accuracy: 9932/10000 (99.32%)\n",
            "\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.025506870821118355 Batch_id=468 Accuracy=99.22: 100%|██████████| 469/469 [00:16<00:00, 28.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0276, Accuracy: 9931/10000 (99.31%)\n",
            "\n",
            "EPOCH: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.02761770784854889 Batch_id=468 Accuracy=99.26: 100%|██████████| 469/469 [00:16<00:00, 28.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0279, Accuracy: 9923/10000 (99.23%)\n",
            "\n",
            "EPOCH: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.008381711319088936 Batch_id=468 Accuracy=99.29: 100%|██████████| 469/469 [00:17<00:00, 27.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0263, Accuracy: 9927/10000 (99.27%)\n",
            "\n",
            "EPOCH: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.010238707065582275 Batch_id=468 Accuracy=99.34: 100%|██████████| 469/469 [00:16<00:00, 28.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0260, Accuracy: 9935/10000 (99.35%)\n",
            "\n",
            "EPOCH: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05624288693070412 Batch_id=468 Accuracy=99.34: 100%|██████████| 469/469 [00:16<00:00, 29.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0247, Accuracy: 9938/10000 (99.38%)\n",
            "\n",
            "EPOCH: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.012332934886217117 Batch_id=468 Accuracy=99.38: 100%|██████████| 469/469 [00:17<00:00, 26.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0243, Accuracy: 9927/10000 (99.27%)\n",
            "\n",
            "EPOCH: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.052587468177080154 Batch_id=468 Accuracy=99.42: 100%|██████████| 469/469 [00:16<00:00, 28.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0306, Accuracy: 9912/10000 (99.12%)\n",
            "\n",
            "EPOCH: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.010730399750173092 Batch_id=468 Accuracy=99.40: 100%|██████████| 469/469 [00:16<00:00, 28.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0251, Accuracy: 9935/10000 (99.35%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TARGET**\n",
        "\n",
        "Keep the parameters less than 8000\n",
        "\n",
        "Reduce the overfitting\n",
        "\n",
        "Reach test accuracy > 99.4\n",
        "\n",
        "**RESULT**\n",
        "\n",
        "Parameters : 8,292\n",
        "\n",
        "Best Training Accuracy : 99.42\n",
        "\n",
        "Best Test Accuracy : 99.38\n",
        "\n",
        "**ANALYSIS**\n",
        "\n",
        "Parameters count is still greater than 8000\n",
        "\n",
        "We could see model overfitting from 12th epoch\n",
        "\n",
        "Even if we continue for few more epochs the model will not reach test accuracy > 99.4\n",
        "\n",
        "**WHAT CAN BE DONE**\n",
        "\n",
        "Reduce the parameter count by reduce the number of feature maps in intermediate layers\n",
        "\n",
        "Add image augmentation\n",
        "\n",
        "Increase the drop out"
      ],
      "metadata": {
        "id": "0zwu1YhWg5fM"
      }
    }
  ]
}