{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nuMQDQBaXEJ"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Phase transformations\n",
        "train_transforms = transforms.Compose([\n",
        "                                       transforms.Resize((28, 28)),\n",
        "                                       transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n"
      ],
      "metadata": {
        "id": "bUsfqDzFadIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
      ],
      "metadata": {
        "id": "cbr1Yw1TafBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJM0vDXMaho3",
        "outputId": "bdc0d9e7-92e6-4c7e-80ec-ba87fdcbd0a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available? True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model3, self).__init__()\n",
        "\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=0),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.ReLU()\n",
        "        ) # INPUT 28*28 | OUTPUT 26*26 | RF 3\n",
        "\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.ReLU()\n",
        "        ) # INPUT 26*26 | OUTPUT 24*24 | RF 5\n",
        "\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.ReLU()\n",
        "        ) # INPUT 24*24 | OUTPUT 22*22 | RF 7\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # INPUT 22*22 | OUTPUT 11*11 | RF 8\n",
        "\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.ReLU()\n",
        "        ) # INPUT 11*11 | OUTPUT 9*9 | RF 12\n",
        "\n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=16, kernel_size=(3, 3), padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU()\n",
        "        ) # INPUT 9*9 | OUTPUT 7*7 | RF 16\n",
        "\n",
        "\n",
        "        self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(3, 3), padding=0),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.ReLU()\n",
        "        ) # INPUT 7*7 | OUTPUT 5*5 | RF 20\n",
        "\n",
        "        self.convblock7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.ReLU()\n",
        "        ) # INPUT 5*5 | OUTPUT 3*3 | RF 24\n",
        "\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(3)\n",
        "        ) # INPUT 3*3 | OUTPUT 1*1 | RF 28\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1) #increased dropout\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.dropout(x)  #dropout\n",
        "        x = self.convblock3(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.dropout(x) #dropout\n",
        "        x = self.convblock6(x)\n",
        "        x = self.convblock7(x)\n",
        "        x = self.gap(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "Q_xdD0JgakDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Model3().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfTBIjD-amj7",
        "outputId": "5cfbdbc4-7cd7-4517-f63e-7b2e6bfa87ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 10, 26, 26]             100\n",
            "       BatchNorm2d-2           [-1, 10, 26, 26]              20\n",
            "              ReLU-3           [-1, 10, 26, 26]               0\n",
            "            Conv2d-4           [-1, 10, 24, 24]             910\n",
            "       BatchNorm2d-5           [-1, 10, 24, 24]              20\n",
            "              ReLU-6           [-1, 10, 24, 24]               0\n",
            "           Dropout-7           [-1, 10, 24, 24]               0\n",
            "            Conv2d-8           [-1, 10, 22, 22]             910\n",
            "       BatchNorm2d-9           [-1, 10, 22, 22]              20\n",
            "             ReLU-10           [-1, 10, 22, 22]               0\n",
            "        MaxPool2d-11           [-1, 10, 11, 11]               0\n",
            "           Conv2d-12             [-1, 10, 9, 9]             910\n",
            "      BatchNorm2d-13             [-1, 10, 9, 9]              20\n",
            "             ReLU-14             [-1, 10, 9, 9]               0\n",
            "           Conv2d-15             [-1, 16, 7, 7]           1,456\n",
            "      BatchNorm2d-16             [-1, 16, 7, 7]              32\n",
            "             ReLU-17             [-1, 16, 7, 7]               0\n",
            "          Dropout-18             [-1, 16, 7, 7]               0\n",
            "           Conv2d-19             [-1, 10, 5, 5]           1,450\n",
            "      BatchNorm2d-20             [-1, 10, 5, 5]              20\n",
            "             ReLU-21             [-1, 10, 5, 5]               0\n",
            "           Conv2d-22             [-1, 10, 3, 3]             910\n",
            "      BatchNorm2d-23             [-1, 10, 3, 3]              20\n",
            "             ReLU-24             [-1, 10, 3, 3]               0\n",
            "        AvgPool2d-25             [-1, 10, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 6,798\n",
            "Trainable params: 6,798\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.50\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.53\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "\n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "metadata": {
        "id": "f-rbVFAzaonu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =  Model3().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "EPOCHS = 15\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no3IeUpdaq7T",
        "outputId": "4f8421fa-a883-440f-d191-d87e2f2b078b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11774060130119324 Batch_id=468 Accuracy=93.16: 100%|██████████| 469/469 [00:32<00:00, 14.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0979, Accuracy: 9820/10000 (98.20%)\n",
            "\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04535025358200073 Batch_id=468 Accuracy=97.93: 100%|██████████| 469/469 [00:31<00:00, 15.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0611, Accuracy: 9875/10000 (98.75%)\n",
            "\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09495209902524948 Batch_id=468 Accuracy=98.35: 100%|██████████| 469/469 [00:32<00:00, 14.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0555, Accuracy: 9862/10000 (98.62%)\n",
            "\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07882190495729446 Batch_id=468 Accuracy=98.51: 100%|██████████| 469/469 [00:31<00:00, 15.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0390, Accuracy: 9909/10000 (99.09%)\n",
            "\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06368156522512436 Batch_id=468 Accuracy=98.65: 100%|██████████| 469/469 [00:31<00:00, 15.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0344, Accuracy: 9914/10000 (99.14%)\n",
            "\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04764308035373688 Batch_id=468 Accuracy=98.70: 100%|██████████| 469/469 [00:31<00:00, 15.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0357, Accuracy: 9906/10000 (99.06%)\n",
            "\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10236049443483353 Batch_id=468 Accuracy=98.84: 100%|██████████| 469/469 [00:30<00:00, 15.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0361, Accuracy: 9898/10000 (98.98%)\n",
            "\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.03791040554642677 Batch_id=468 Accuracy=98.91: 100%|██████████| 469/469 [00:30<00:00, 15.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0311, Accuracy: 9922/10000 (99.22%)\n",
            "\n",
            "EPOCH: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09331950545310974 Batch_id=468 Accuracy=98.93: 100%|██████████| 469/469 [00:31<00:00, 14.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0290, Accuracy: 9913/10000 (99.13%)\n",
            "\n",
            "EPOCH: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05725391209125519 Batch_id=468 Accuracy=99.00: 100%|██████████| 469/469 [00:30<00:00, 15.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0259, Accuracy: 9932/10000 (99.32%)\n",
            "\n",
            "EPOCH: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.02974691428244114 Batch_id=468 Accuracy=99.03: 100%|██████████| 469/469 [00:30<00:00, 15.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0274, Accuracy: 9923/10000 (99.23%)\n",
            "\n",
            "EPOCH: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.053218577057123184 Batch_id=468 Accuracy=99.04: 100%|██████████| 469/469 [00:31<00:00, 14.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0242, Accuracy: 9931/10000 (99.31%)\n",
            "\n",
            "EPOCH: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.02607146091759205 Batch_id=468 Accuracy=99.10: 100%|██████████| 469/469 [00:30<00:00, 15.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0242, Accuracy: 9930/10000 (99.30%)\n",
            "\n",
            "EPOCH: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.023845404386520386 Batch_id=468 Accuracy=99.13: 100%|██████████| 469/469 [00:31<00:00, 14.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0230, Accuracy: 9932/10000 (99.32%)\n",
            "\n",
            "EPOCH: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.03304101154208183 Batch_id=468 Accuracy=99.15: 100%|██████████| 469/469 [00:30<00:00, 15.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0236, Accuracy: 9927/10000 (99.27%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TARGET**\n",
        "\n",
        "Keep the parameters less than 8000\n",
        "\n",
        "Reduce the overfitting\n",
        "\n",
        "Reach test accuracy > 99.4\n",
        "\n",
        "**RESULT**\n",
        "\n",
        "Parameters :  6,798\n",
        "\n",
        "Best Training Accuracy : 99.15\n",
        "\n",
        "Best Test Accuracy : 99.32\n",
        "\n",
        "**ANALYSIS**\n",
        "\n",
        "Total parameters has been kept under 8000\n",
        "\n",
        "The model is not overfitting any more\n",
        "\n",
        "Required test accuracy > 99.4% has not been reached yet\n",
        "\n",
        "**WHAT CAN BE DONE**\n",
        "\n",
        "Gradually increase the number of feature maps in intermediate layers but still keep the count under 8000\n",
        "\n",
        "Introducing scheduler like StepLR to adjust the learning rate during training\n",
        "\n",
        "Trying Adam optimizer with lower learning rate for optimization"
      ],
      "metadata": {
        "id": "NSt_-zPGWxf1"
      }
    }
  ]
}